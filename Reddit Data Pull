import pandas as pd
files = ['VaxxHesAntiCon.csv', 'VaxxHesAntiTop.csv', 'VaxxHesUVTop.csv', 'VaxxHesUVCon.csv', 'VaxxHesDVTop.csv', 'VaxxHesDVCon.csv', 'VaxxHesVHTop.csv', 'VaxxHesVHCon.csv', 'VaxxHesVMTop.csv', 'VaxxHesVMCon.csv' ]
df = pd.DataFrame()
for file in files:
    data = pd.read_csv(file)
    df = pd.concat([df, data], axis=0)
df.to_csv('merged_reddit_files.csv', index=False)

reddit_posts = pd.read_csv('merged_reddit_files.csv', error_bad_lines=False);

data_text = reddit_posts[['Title']]
data_text['index'] = data_text.index
documents = data_text

print(len(documents))
print(documents[:20])

import gensim
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS
from nltk.stem import WordNetLemmatizer, SnowballStemmer
from nltk.stem.porter import *
import numpy as np
np.random.seed(2018)
import nltk
nltk.download('wordnet')
from nltk import FreqDist

import pandas as pd
from gensim.utils import simple_preprocess
from nltk.stem import WordNetLemmatizer

def lemmatize_stemming(text):
    return WordNetLemmatizer().lemmatize(text, pos='v')

def preprocess_corpus(docs):
    processed_corpus = []
    for index, row in documents.iterrows():
        title = row['Title']
        tokens = []
        for token in simple_preprocess(title):
            if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:
                tokens.append(lemmatize_stemming(token))
        processed_corpus.append((tokens))
    return processed_corpus

# Assuming `df` is your DataFrame with "Title" column and index

# Remove the index column
df_without_index = documents.drop(columns=['index'])

# Preprocess the corpus
processed_corpus = preprocess_corpus(df_without_index)

# Print the processed corpus (processed words only)
for tokens in processed_corpus:
    print(f"Processed Words: {tokens}")

import nltk
import numpy as np
from nltk import FreqDist
import matplotlib.pyplot as plt
nltk.download('punkt')

from itertools import chain

# Example lists to be merged
processed_corpus

merged_list = []
for sublist in processed_corpus :
    merged_list.extend(sublist)

print(merged_list)

fdist = FreqDist(merged_list)
words_to_remove = fdist.most_common(10)
words_to_remove

def remove_words_from_corpus(corpus, words_to_remove):
    processed_corpus = []
    for tokens in corpus:
        filtered_tokens = [token for token in tokens if token not in words_to_remove]
        processed_corpus.append(filtered_tokens)
    return processed_corpus, filtered_tokens

# Assuming `processed_corpus` is your processed corpus list and `words_to_remove` is your list of words to remove

# Define words to remove
words_to_remove = ['covid', 'vaccine', 'vaccines', 'people', 'vaccinate', 'unvaccinated', 'anti', 'cause', 'deaths', 'study']  # List of words to remove

# Make a copy of the processed corpus
filtered_corpus = processed_corpus[:]

# Loop through the words to remove
for word in words_to_remove:
    filtered_corpus, _ = remove_words_from_corpus(filtered_corpus, [word])

# Print the filtered corpus (filtered words)
for tokens in filtered_corpus:
    print(f"Filtered Words: {tokens}")

import nltk
import numpy as np
from nltk import FreqDist
import matplotlib.pyplot as plt
nltk.download('punkt')

from itertools import chain

 # Example lists to be merged
filtered_corpus

merged_list2 = []
for sublist in filtered_corpus :
    merged_list2.extend(sublist)

print(merged_list2)

fdist_new = FreqDist(merged_list2)
top_ten_new = fdist_new.most_common(10)
print(top_ten_new) 

#Shows the frequency distribution for top 30 tokens after top 10 were removed
fdist_new.plot(40,title='Frequency distribution for 30 most common tokens in our text collection')

dictionary = gensim.corpora.Dictionary(filtered_corpus)
count = 0
for k, v in dictionary.iteritems():
    print(k, v)
    count += 1
    if count > 10:
        break

bow_corpus = [dictionary.doc2bow(doc) for doc in filtered_corpus]
bow_corpus[1237] 

from gensim import corpora, models
tfidf = models.TfidfModel(bow_corpus)
corpus_tfidf = tfidf[bow_corpus]
from pprint import pprint
for doc in corpus_tfidf:
    pprint(doc)
    break
              
lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=5, id2word=dictionary, passes=10, workers=2)
for idx, topic in lda_model.print_topics(-1):
    print('Topic: {} \nWords: {}'.format(idx, topic))

lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=5, id2word=dictionary, passes=2, workers=4)
for idx, topic in lda_model_tfidf.print_topics(-1):
    print('Topic: {} Word: {}'.format(idx, topic))

filtered_corpus[1237]

for index, score in sorted(lda_model[bow_corpus[1237]], key=lambda tup: -1*tup[1]):
    print("\nScore: {}\t \nTopic: {}".format(score, lda_model.print_topic(index, 10)))    

from gensim.models import CoherenceModel
import matplotlib.pyplot as plt
import re   

def format_topics_sentences(ldamodel=None, corpus=bow_corpus, texts=data):
    # Init output
    sent_topics_df = pd.DataFrame()

    # Get main topic in each document
    for i, row_list in enumerate(ldamodel[bow_corpus]):
        row = row_list[0] if ldamodel.per_word_topics else row_list
        # print(row)
        row = sorted(row, key=lambda x: (x[1]), reverse=True)
        # Get the Dominant topic, Perc Contribution and Keywords for each document
        for j, (topic_num, prop_topic) in enumerate(row):
            if j == 0:  # => dominant topic
                wp = ldamodel.show_topic(topic_num)
                topic_keywords = ", ".join([word for word, prop in wp])
                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)
            else:
                break
    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']

    # Add original text to the end of the output
    contents = pd.Series(texts)
    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)
    return(sent_topics_df)


df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=bow_corpus, texts=filtered_corpus)

# Format
df_dominant_topic = df_topic_sents_keywords.reset_index()
df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']
df_dominant_topic.head(5)

# Display setting to show more characters in column
pd.options.display.max_colwidth = 100

sent_topics_sorteddf_mallet = pd.DataFrame()
sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')

for i, grp in sent_topics_outdf_grpd:
    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet,
                                             grp.sort_values(['Perc_Contribution'], ascending=False).head(1)],
                                            axis=0)

# Reset Index
sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)

# Format
sent_topics_sorteddf_mallet.columns = ['Topic_Num', "Topic_Perc_Contrib", "Keywords", "Representative Text"]

# Show
sent_topics_sorteddf_mallet.head(10)

doc_lens = [len(d) for d in df_dominant_topic.Text]

# Plot
plt.figure(figsize=(10,10), dpi=160)
plt.hist(doc_lens, bins = 320, color='navy')
plt.text(750, 100, "Mean   : " + str(round(np.mean(doc_lens))))
plt.text(750,  90, "Median : " + str(round(np.median(doc_lens))))
plt.text(750,  80, "Stdev   : " + str(round(np.std(doc_lens))))
plt.text(750,  70, "1%ile    : " + str(round(np.quantile(doc_lens, q=0.01))))
plt.text(750,  60, "99%ile  : " + str(round(np.quantile(doc_lens, q=0.99))))

plt.gca().set(xlim=(0, 32), ylabel='Number of Documents', xlabel='Document Word Count')
plt.tick_params(size=16)
plt.xticks(np.linspace(0,32,9))
plt.title('Distribution of Document Word Counts', fontdict=dict(size=22))
# Remove extra white space to the right
fig.subplots_adjust(right=0.15)
plt.show()

import seaborn as sns
import matplotlib.colors as mcolors
cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'

fig, axes = plt.subplots(2,2,figsize=(16,14), dpi=160, sharex=True, sharey=True)

for i, ax in enumerate(axes.flatten()):
    df_dominant_topic_sub = df_dominant_topic.loc[df_dominant_topic.Dominant_Topic == i, :]
    doc_lens = [len(d) for d in df_dominant_topic_sub.Text]
    ax.hist(doc_lens, bins = 50, color=cols[i])
    ax.tick_params(axis='y', labelcolor=cols[i], color=cols[i])
    sns.kdeplot(doc_lens, color="black", shade=False, ax=ax.twinx())
    ax.set(xlim=(0, 50), xlabel='Document Word Count')
    ax.set_ylabel('Number of Documents', color=cols[i])
    ax.set_title('Topic: '+str(i), fontdict=dict(size=16, color=cols[i]))

fig.tight_layout()
fig.subplots_adjust(top=0.90)
plt.xticks(np.linspace(0,50,9))
fig.suptitle('Distribution of Document Word Counts by Dominant Topic', fontsize=22)
plt.show()

# 1. Wordcloud of Top N words in each topic
from matplotlib import pyplot as plt
from wordcloud import WordCloud, STOPWORDS
import matplotlib.colors as mcolors


cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'

cloud = WordCloud(stopwords=stop_words,
                  background_color='white',
                  width=2500,
                  height=1800,
                  max_words=10,
                  colormap='tab10',
                  color_func=lambda *args, **kwargs: cols[i],
                  prefer_horizontal=1.0)

topics = lda_model.show_topics(formatted=False)

fig, axes = plt.subplots(2, 2, figsize=(10,10), sharex=True, sharey=True)

for i, ax in enumerate(axes.flatten()):
    fig.add_subplot(ax)
    topic_words = dict(topics[i][1])
    cloud.generate_from_frequencies(topic_words, max_font_size=300)
    plt.gca().imshow(cloud)
    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))
    plt.gca().axis('off')


plt.subplots_adjust(wspace=0, hspace=0)
plt.axis('off')
plt.margins(x=0, y=0)
plt.tight_layout()
plt.show()


from collections import Counter
topics = lda_model.show_topics(formatted=False)
data_flat = [w for w_list in filtered_corpus for w in w_list]
counter = Counter(data_flat)

out = []
for i, topic in topics:
    for word, weight in topic:
        out.append([word, i , weight, counter[word]])

df = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])

# Plot Word Count and Weights of Topic Keywords
fig, axes = plt.subplots(2, 2, figsize=(16,10), sharey=True, dpi=160)
cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]
for i, ax in enumerate(axes.flatten()):
    ax.bar(x='word', height="word_count", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.5, alpha=0.3, label='Word Count')
    ax_twin = ax.twinx()
    ax_twin.bar(x='word', height="importance", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.2, label='Weights')
    ax.set_ylabel('Word Count', color=cols[i])
    ax_twin.set_ylim(0, 0.030); ax.set_ylim(0, 3500)
    ax.set_title('Topic: ' + str(i), color=cols[i], fontsize=16)
    ax.tick_params(axis='y', left=False)
    ax.set_xticklabels(df.loc[df.topic_id==i, 'word'], rotation=30, horizontalalignment= 'right')
    ax.legend(loc='upper left'); ax_twin.legend(loc='upper right')

fig.tight_layout(w_pad=2)
fig.suptitle('Word Count and Importance of Topic Keywords', fontsize=22, y=1.05)
plt.show()


        
                                            
              
